{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1ba2fa",
   "metadata": {},
   "source": [
    "# Logistic Regression: Cost Function and Gradient Implementation\n",
    "\n",
    "In this coding exercise, you will be implementing the cost function and gradient function for linear regression. The cost function measures the error between the predicted values and the actual values, while the gradient function calculates the derivatives of the cost function with respect to the model parameters. You will use a synthetic dataset to test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df09a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fce3ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's simulate some data\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate two clouds of points from normal distributions\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate points for the first group\n",
    "mean1 = [1, -1]\n",
    "cov1 = [[1, 0], [0, 1]]\n",
    "cloud1 = np.random.multivariate_normal(mean1, cov1, n_samples)\n",
    "\n",
    "# Generate points for the second group\n",
    "mean2 = [-1, 1]\n",
    "cov2 = [[1, 0], [0, 1]]\n",
    "cloud2 = np.random.multivariate_normal(mean2, cov2, n_samples)\n",
    "\n",
    "# Combine the two groups to create the feature matrix X\n",
    "X = np.vstack((cloud1, cloud2))\n",
    "\n",
    "# Generate the target variable y\n",
    "y = np.concatenate((np.zeros(n_samples), np.ones(n_samples)))\n",
    "\n",
    "# TODO: Split the data into training and testing sets\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "# Reshape the target variables. What is the -1 used for?\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print()\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3bdcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.title('Dataset Visualization')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(ticks=[0, 1], label='Class')\n",
    "plt.grid()\n",
    "\n",
    "# TODO: Plot train data\n",
    "plt.scatter(# TODO[:, 0], # TODO[:, 1], c=y_train, cmap='viridis', edgecolor='black', linewidth=1, marker='s', label='Train Data')\n",
    "\n",
    "# TODO: Plot test data\n",
    "plt.scatter(# TODO[:, 0], # TODO[:, 1], c=y_test, cmap='viridis', edgecolor='black', linewidth=1, marker='^', label='Test Data')\n",
    "\n",
    "# Set legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761c708",
   "metadata": {},
   "source": [
    "# Logistic Regression Formulation\n",
    "\n",
    "Logistic regression is a statistical method for predicting binary classes. The output is a probability that the given input point belongs to a certain class.\n",
    "\n",
    "## Hypothesis Function\n",
    "\n",
    "The hypothesis function in logistic regression is defined as:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `h_\\theta(x)` is the predicted probability that the output is 1.\n",
    "- `x` is the feature vector.\n",
    "- `\\theta` is the parameter vector.\n",
    "- `e` is the base of the natural logarithm.\n",
    "\n",
    "This function is also known as the sigmoid function. It maps any real value into the range of 0 to 1, making it suitable for a probability estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the predict function for logistic regression\n",
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Predict the target variable using the logistic regression model.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, p), where n is the number of samples and p is the number of features.\n",
    "    theta (numpy.ndarray): Model parameters of shape (p, 1).\n",
    "\n",
    "    Returns:\n",
    "    probabilities (numpy.ndarray): Predicted probabilities of shape (n, 1).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Define probabilitiesof the logistic regression\n",
    "    probabilities = # TODO\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc37649",
   "metadata": {},
   "source": [
    "# Mathematical Formulation for Logistic Regression\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "The cost function in logistic regression, also known as the logistic loss, is defined as:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `n` is the number of training examples.\n",
    "- `x^{(i)}` represents the feature vector of the `i`-th training example.\n",
    "- `y^{(i)}` is the actual label of the `i`-th training example.\n",
    "- `h_\\theta(x)` is the hypothesis function for logistic regression, defined as \\( h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}} \\).\n",
    "- `\\theta` represents the model parameters.\n",
    "\n",
    "## Gradient Function\n",
    "\n",
    "The gradient of the cost function for logistic regression is a vector where each element is the partial derivative of the cost function with respect to the corresponding parameter:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "For vectorized implementation:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{n} X^T (h_\\theta(X) - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- `X` is the matrix of input features.\n",
    "- `\\mathbf{y}` is the vector of actual labels.\n",
    "- `\\nabla_\\theta J(\\theta)` is the gradient of the cost function with respect to `\\theta`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1233dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the cost function for logistic regression\n",
    "def cost_function(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost function for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, p), where n is the number of samples and p is the number of features.\n",
    "    y (numpy.ndarray): Target values of shape (n, 1).\n",
    "    theta (numpy.ndarray): Model parameters of shape (p, 1).\n",
    "\n",
    "    Returns:\n",
    "    cost (float): Cost value corresponding to the logistic loss.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(y)\n",
    "\n",
    "    # TODO: Calculate probabilities\n",
    "    probabilities = # TODO\n",
    "\n",
    "    # TODO: Compute the cost function (- log-likelihood)\n",
    "    cost = # TODO\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "# TODO: Implement the gradient function for logistic regression\n",
    "def gradient_function(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cost function for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n, p), where n is the number of samples and p is the number of features.\n",
    "    y (numpy.ndarray): Target values of shape (n, 1).\n",
    "    theta (numpy.ndarray): Model parameters of shape (p, 1).\n",
    "\n",
    "    Returns:\n",
    "    gradient (numpy.ndarray): Gradient vector of shape (p, 1).\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(y)\n",
    "\n",
    "    # TODO: Calculate probabilities\n",
    "    probabilities = # TODO\n",
    "\n",
    "    # TODO: Compute the gradient of the cost function\n",
    "    gradient = # TODO\n",
    "\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# TODO: Implement the train function to learn the weights of the logistic regression model using gradient descent\n",
    "def train_model(X_train, y_train, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Train the logistic regression model using gradient descent optimization.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (numpy.ndarray): Feature matrix of shape (n, p) for training data.\n",
    "    y_train (numpy.ndarray): Target values of shape (n, 1) for training data.\n",
    "    learning_rate (float): Learning rate for gradient descent.\n",
    "    num_iterations (int): Number of iterations for training.\n",
    "\n",
    "    Returns:\n",
    "    theta (numpy.ndarray): Model parameters of shape (p, 1).\n",
    "    costs_train (list): List of training costs at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X_train.shape\n",
    "    theta = np.zeros((p, 1))\n",
    "    costs_train = []\n",
    "\n",
    "    # TODO: Implement the optimization part\n",
    "    for _ in range(num_iterations):\n",
    "        gradient = # TODO\n",
    "        theta -= # TODO\n",
    "\n",
    "        cost_train = # TODO\n",
    "        costs_train.append(cost_train[0, 0])\n",
    "\n",
    "    return theta, costs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate artificial data for demonstration\n",
    "np.random.seed(0)\n",
    "\n",
    "# TODO: Train the logistic regression model\n",
    "learning_rate = # TODO\n",
    "num_iterations = # TODO\n",
    "theta_hat, costs_train = # TODO:\n",
    "\n",
    "# TODO: Make predictions on the training data\n",
    "probability_threshold = 1/2\n",
    "\n",
    "probabilities_train = # TODO\n",
    "probabilities_test = # TODO\n",
    "\n",
    "y_train_pred = (# TODO).astype(int)\n",
    "y_test_pred = (# TODO).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the training costs\n",
    "plt.plot(# TODO)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Training Cost over Iterations')z\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the precision function\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the precision score for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): True target values of shape (n,).\n",
    "    y_pred (numpy.ndarray): Predicted target values of shape (n,).\n",
    "\n",
    "    Returns:\n",
    "    precision (float): Precision score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Compute the number of true positives\n",
    "    true_positives = # TODO\n",
    "\n",
    "    # TODO: Compute the number of false positives\n",
    "    false_positives = # TODO\n",
    "    \n",
    "    # TODO: Compute the precision\n",
    "    precision = # TODO\n",
    "    \n",
    "    return precision\n",
    "\n",
    "# TODO: Define the accuracy function\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the accuracy score for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): True target values of shape (n,).\n",
    "    y_pred (numpy.ndarray): Predicted target values of shape (n,).\n",
    "\n",
    "    Returns:\n",
    "    accuracy (float): Accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Compute the number of correct predictions\n",
    "    correct_predictions = # TODO\n",
    "    \n",
    "    # TODO: Compute the number of total predictions\n",
    "    total_predictions = # TODO\n",
    "    \n",
    "    # TODO: Compute the accuracy\n",
    "    accuracy = # TODO\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875476e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Compute precision for the training dataset\n",
    "train_precision = # TODO\n",
    "\n",
    "# TODO: Compute precision for the test dataset\n",
    "test_precision = # TODO\n",
    "\n",
    "# TODO: Compute accuracy for the training dataset\n",
    "train_accuracy = # TODO\n",
    "\n",
    "# TODO: Compute accuracy for the test dataset\n",
    "test_accuracy = # TODO\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Precision:\", train_precision)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Precision:\", test_precision)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# What are your conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d587e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): True target values of shape (n,).\n",
    "    y_pred (numpy.ndarray): Predicted target values of shape (n,).\n",
    "\n",
    "    Returns:\n",
    "    confusion_matrix (numpy.ndarray): Confusion matrix of shape (2, 2).\n",
    "    \"\"\"\n",
    "    true_positive = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n",
    "    true_negative = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n",
    "    false_positive = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n",
    "    false_negative = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n",
    "    \n",
    "    confusion_matrix = np.array([[true_negative, false_positive], [false_negative, true_positive]])\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    cm (numpy.ndarray): Confusion matrix.\n",
    "    classes (list): List of class labels.\n",
    "    title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(shrink=0.37)\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix for the training dataset\n",
    "train_cm = calculate_confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Compute the confusion matrix for the test dataset\n",
    "test_cm = calculate_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix for the training dataset\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_confusion_matrix(train_cm, classes=['Class 0', 'Class 1'], title='Confusion Matrix - Training Dataset')\n",
    "\n",
    "# Plot the confusion matrix for the test dataset\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_confusion_matrix(test_cm, classes=['Class 0', 'Class 1'], title='Confusion Matrix - Test Dataset')\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ea1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or Use Sklearn's implementation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Compute the confusion matrix for the training dataset\n",
    "train_cm = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Compute the confusion matrix for the test dataset\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Plot the confusion matrix for the training dataset\n",
    "print(\"Training Confusion Matrix\")\n",
    "ConfusionMatrixDisplay(train_cm, display_labels=['Class 0', 'Class 1']).plot()\n",
    "\n",
    "# Plot the confusion matrix for the test dataset\n",
    "print(\"Test Confusion Matrix\")\n",
    "ConfusionMatrixDisplay(test_cm, display_labels=['Class 0', 'Class 1']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712242a9",
   "metadata": {},
   "source": [
    "# Logistic Regression:  Breast cancer case study\n",
    "\n",
    "In this exercise, we will fit logistic regression using our implementation and compare it that provided by scikit-learn. We will use the Breast Cancer Wisconsin dataset, split it into training and testing sets, and then train and evaluate the models' performance using precision and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b52d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# TODO: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = # TODO\n",
    "\n",
    "# TODO: Scale the features using StandardScaler\n",
    "scaler = # TODO\n",
    "X_train_scaled = # TODO\n",
    "X_test_scaled = # TODO\n",
    "\n",
    "# Reshape the target variables\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1378c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the dataset about? print data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a3773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Train the logistic regression using custom implementation\n",
    "learning_rate = # TODO\n",
    "num_iterations = # TODO\n",
    "theta_hat, costs_train = # TODO\n",
    "\n",
    "# TODO: Create a logistic regression object\n",
    "lr_sklearn = # TODO\n",
    "\n",
    "# TODO: Train the model on the scaled training data\n",
    "\n",
    "# TODO: Compute the predictions for both models\n",
    "y_pred_custom = (# TODO).astype(int)\n",
    "y_pred_sklearn = # TODO\n",
    "\n",
    "# TODO: Compute the precision and accuracy for both models\n",
    "precision_custom = # TODO\n",
    "accuracy_custom = # TODO\n",
    "\n",
    "precision_sklearn = # TODO\n",
    "accuracy_sklearn = # TODO\n",
    "\n",
    "# Print the results\n",
    "print(\"Custom Logistic Regression:\")\n",
    "print(f\"Precision: {precision_custom}\")\n",
    "print(f\"Accuracy: {accuracy_custom}\")\n",
    "\n",
    "print(\"\\nScikit-learn Logistic Regression:\")\n",
    "print(f\"Precision: {precision_sklearn}\")\n",
    "print(f\"Accuracy: {accuracy_sklearn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
