{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1698480491490,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"xp0xEaxQ9IdQ"},"outputs":[],"source":["from IPython.display import clear_output"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16439,"status":"ok","timestamp":1698480783305,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"GMpV5X0P9RJ-"},"outputs":[],"source":["# Download the required libraries (needed when running outside colab where the environment doesn't come pre-loaded with libraries)\n","\n","%pip install numpy\n","%pip install scikit-learn\n","%pip install matplotlib\n","\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-h6xg45AYUmF"},"outputs":[],"source":["import math\n","\n","# TODO: Import numpy and matplotlib.pyplot as np and plt respectively\n","\n","# Sklearn has implementations of multiple types of models. We'll be using LinearRegression API in it\n","# For documentation on LinearRegression, visit here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n","from sklearn.linear_model import LinearRegression"]},{"cell_type":"markdown","metadata":{"id":"37bi0tehXm-p"},"source":["#Contents:\n","\n","1. Implement 1 polynomial degree Linear Regression model from scratch (using numpy)\n","2. Implement the same model using sklearn\n","3. Take a complex function and try fitting a multi-polynomial degree Linear regression model on it.\n","\n","\n","You need to know:\n","\n","1. **numpy** (for impelementation)\n","2. a little bit of **matplotlib** (for visualization)\n","\n","\n","Good to have knowledge of:\n","\n","1. Sklearn (details of the functions is given anyways)"]},{"cell_type":"markdown","metadata":{"id":"Shzta92y9oZI"},"source":["## Implementing Linear Regression from Scratch (Using numpy)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":1168,"status":"ok","timestamp":1698480784462,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"PwIENVuG99b8","outputId":"05190c23-715f-4084-d634-0130c472adf3"},"outputs":[],"source":["# Let's make some custom points (which would act as our dataset)\n","# starting with a function with highest polynomial degree of 1\n","\n","# y = w0 + w1*x1\n","\n","w0 = 12  # bias\n","w1 = 15  # first degree co-efficient\n","noise_scale_factor = 5  # the higher this is, the rougher and farther the noisy output is from the best fit\n","num_points = 50  # number of samples in data\n","\n","x_points = np.linspace(0, 1, num_points)  # num_points points from 0 to 1\n","y_points_no_noise = w0 + w1*x_points  # The actual features which we'd feed to model will have slight noise\n","noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)  # Look at the explanation below to see how this is calculated\n","\n","y_points = y_points_no_noise + noise\n","\n","plt.plot(x_points, y_points_no_noise, label='No noise output (what we want as best fit)')\n","plt.scatter(x_points, y_points, c=\"Orange\", label='Noisy output (labels fed to model)')\n","_, ylim_top = plt.ylim()\n","plt.ylim(0, ylim_top)  # 0 bottom makes it easy to visualize bias term\n","\n","plt.xlabel('X-points (features)')\n","plt.ylabel('Y-points (output / labels)')\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"f5BL-GJbE-jm"},"source":["### Explanation:\n","\n","```python\n","noise_scale_factor = 5\n","noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)\n","y_points = y_points_no_noise + noise\n","```\n","\n","- np.random.rand(*y_points_no_noise) generates an array of random values in range [0, 1) of the same shape as y_points_no_noise\n","- we subtract 0.5 from it to bring it to range [-0.5, 0.5) (so there are also some values below the line after noise is added)\n","- it might be possible that the values of w0, w1 or y_points is big enough for a noise point of [-0.5, 0.5) to not make much dent (for example, if a label is 1000, changing it to 1000.5 or 999.5 doesn't make much different) so we scale it with a scaling factor\n","- The calculated noise is added to noise-less y_points"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1698480784463,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Q9jdmjvX_ept","outputId":"0f49f878-568e-46f2-815d-bab51404950f"},"outputs":[],"source":["# Custom calculating best fit\n","\n","num_bases = 1  # number of coefficient to polynomial terms (not including bias). We'll pick 1 since our original function had highest order term of 1\n","\n","A = [x_points**i for i in range(num_bases+1)]  # +1 in num_bases for bias\n","A = np.array(A).T  # (m, n) matrix where m is number of samples and n is number of params (including bias). In our case, n is 2\n","\n","\n","# Solve for w to find our params\n","# Upper case: matrices. Lower case: vectors\n","\n","#  Aw = y\n","#  Aᵀ(Aw) = Aᵀy  =>  AᵀAw = Aᵀy\n","#  w = (AᵀA)⁻¹Aᵀy\n","\n","# Hint: Remember how we use matrix multiplication (@) and inverse (np.linalg.inv)\n","# to find the optimal weights in linear regression.\n","### TODO\n","w = \n","\n","# This line calculates the predicted values 'y_pred' using the matrix 'A' and the weights 'w'.\n","### TODO\n","y_pred = \n","\n","# Visualize the results\n","\n","plt.plot(x_points, y_points_no_noise, label='data points without the noise (our goal)')\n","plt.plot(x_points, y_points, 'x', label='points in data (with noise)')\n","plt.plot(x_points, y_pred, label='prediction')\n","\n","_, ylim_top = plt.ylim()\n","plt.ylim(0, ylim_top+10)  # 0 bottom makes it easy to visualize bias term (intercept)\n","\n","plt.xlabel('X-points (features)')\n","plt.ylabel('Y-points (output / labels)')\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1698480784464,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"RsSuO-qfx5gB","outputId":"eb93d6e9-1306-4b37-acc1-baf5087144d0"},"outputs":[],"source":["print(f'Actual weight values we used: {(w0, w1)}')\n","print(f'Values calculated by Model: {tuple(w)}')"]},{"cell_type":"markdown","metadata":{"id":"mDMgBfr0XmaA"},"source":["## Let's now use sklearn to do this"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1698480784464,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"U6yN5mOahfDz"},"outputs":[],"source":["model = LinearRegression(fit_intercept=True)\n","model = model.fit(x_points.reshape(-1, 1), y_points)  # fit is used to train the model on the data."]},{"cell_type":"markdown","metadata":{"id":"ds0xF__V2A_b"},"source":["### Explanation\n","\n","**fit_intercept=True**: This is used to tell the model that we also expect to calculate the intercept.\n","For cases where the data is centralized during pre-processing, this value can be False since the intercept in that case is 0.\n","But since that's not the case with us, we want the model to calculate it\n","\n","**x_points.reshape(-1, 1)**: Sklearn expects input features to be of the shape (m, n) where m is the number of samples and n is the number of features.\n","Since the array we generated is of the shape (n,), we need to reshape it to (n, 1). This does not change the values, only the shape. Sklearn throws an error if we don't do this"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1698480784465,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"9fU8fYejyYk7"},"outputs":[],"source":["y_pred = model.predict(x_points.reshape(-1, 1))  # Predict is used to prediction labels/outputs from feature inputs."]},{"cell_type":"markdown","metadata":{"id":"gNu0XUo04t-a"},"source":["Note: Normally it's a good idea to predict a separate set of data not seen during training (validation data), but here we're just looking at the basic implementation using sklearn so training data would also do"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1698480784466,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"7p-tfpPhznBX","outputId":"40c13bd5-2d0f-4943-864f-e6189ded0e82"},"outputs":[],"source":["plt.plot(x_points, y_points_no_noise, label='data points without the noise (our goal)')\n","plt.plot(x_points, y_points, 'x', label='points in data (with noise)')\n","plt.plot(x_points, y_pred, label='prediction')\n","\n","_, ylim_top = plt.ylim()\n","plt.ylim(0, ylim_top+10)\n","plt.xlabel('X-points (features)')\n","plt.ylabel('Y-points (output / labels)')\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1698480784466,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"Bn03H6mTzqG5","outputId":"3df34e09-9aab-4e43-f74c-d041192574d5"},"outputs":[],"source":["print(f'Expected coefficients/matrices:           {(w0, w1)}')\n","\n","# we have to do [0] on coef_ because coef_ itself is a matrix of all the coeffifients. In our case, coef_ just has one length but it's still an array.\n","# [0] brings out the value from array which means better printing\n","print(f'Sklearn calculated coeffidients/matrices: {(model.intercept_, model.coef_[0])}')"]},{"cell_type":"markdown","metadata":{"id":"ba-4Ox1r7bN4"},"source":["## Let's step up the game with more complex functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":1172,"status":"ok","timestamp":1698480785610,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"jYpUaxrn1VB-","outputId":"f74cb33f-9b9d-43a6-a5bb-31d3165d7c8e"},"outputs":[],"source":["noise_scale_factor = 1.5\n","num_points = 100\n","sin_wave_ub = 3  # upper bound of sin wave y output. negative of this value would also be lower bound. (negative value here would make this a cosine wave)\n","\n","x_points = np.linspace(-1, 1, num_points)\n","y_points_no_noise = sin_wave_ub * np.sin(x_points*math.pi)\n","\n","noise = noise_scale_factor * (np.random.rand(*y_points_no_noise.shape)-0.5)\n","y_points = y_points_no_noise + noise\n","\n","plt.plot(x_points, y_points_no_noise, label='No noise output')\n","plt.scatter(x_points, y_points, c = \"orange\",label='Noisy output (labels fed to model)')\n","\n","plt.xlabel('X-points (features)')\n","plt.ylabel('Y-points (output / labels)')\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2Ut2OpDhZTFX"},"source":["#### Custom fitting with variable number of bases"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9655,"status":"ok","timestamp":1698480795246,"user":{"displayName":"Telha Bin Bilal","userId":"04147761314111353462"},"user_tz":-300},"id":"GTAmcROa-zg0","outputId":"1722dc9b-42de-4baf-d54a-dc875bc17d26"},"outputs":[],"source":["# Custom calculating best fit\n","\n","\n","min_num_bases = 1\n","max_num_bases = 6\n","\n","\n","assert min_num_bases <= max_num_bases, \"min_num_bases must be lesser or equal to max_num_bases. :|\"\n","\n","bases_preds = []\n","\n","for num_bases in range(min_num_bases, max_num_bases+1):\n","\n","  A = [x_points**i for i in range(num_bases+1)]\n","  A = np.array(A).T\n","\n","  # Solve for w to find our params\n","  # Upper case: matrices. Lower case: vectors\n","\n","  #  Aw = y\n","  #  Aᵀ(Aw) = Aᵀy  =>  AᵀAw = Aᵀy\n","  #  w = (AᵀA)⁻¹Aᵀy\n","\n","  # Hint: Consider how the transpose of matrix 'A' and the inverse function play a role \n","  # in finding the optimal weights 'w'. This is a key step in minimizing the cost function.\n","  ### TODO\n","  w = \n","\n","  # Now that we have our weights, this line is used to make predictions. \n","  # Hint: Reflect on how matrix multiplication here helps in applying the learned \n","  # weights 'w' to our feature matrix 'A' to predict the output 'y_pred'.\n","  ### TODO\n","  y_pred = \n","\n","  bases_preds.append(y_pred)\n","\n","\n","fig, axes = plt.subplots(len(bases_preds), 1, figsize=(20, 10*len(bases_preds)))\n","\n","for i, (num_bases, preds) in enumerate(zip(range(min_num_bases, max_num_bases+1), bases_preds)):\n","\n","    axes[i].set_title(f'Num Bases = {num_bases}')\n","\n","    # Visualize the results\n","    axes[i].plot(x_points, y_points_no_noise, label='data points without the noise (our goal)')\n","    axes[i].plot(x_points, y_points, 'x',     label='points in data (with noise)')\n","    axes[i].plot(x_points, bases_preds[i],    label='prediction')\n","\n","    axes[i].set_xlabel('X-points (features)')\n","    axes[i].set_ylabel('Y-points (output / labels)')\n","\n","    axes[i].legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DV1p1u_CYPPL"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNKemDy5mCKDCxC6+kWM9VB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
